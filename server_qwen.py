# ===== LLM-CONTEXT-START: IMPORTS AND CONFIG =====
# @LLM-CONTEXT: åŸºç¤é…ç½®å’Œå°å…¥ - LLM éœ€è¦äº†è§£çš„ä¾è³´å’Œé…ç½®
from fastapi import FastAPI, Request, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.responses import StreamingResponse, HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import httpx
import os
import json
from dotenv import load_dotenv
import time
import edge_tts
from gtts import gTTS  # Google TTS as fallback
import asyncio
import io
from pydantic import BaseModel

# Azure Cognitive Services TTS (for Cantonese)
try:
    import azure.cognitiveservices.speech as speechsdk
    AZURE_TTS_AVAILABLE = True
except ImportError:
    AZURE_TTS_AVAILABLE = False
    logger.warning("Azure Speech SDK not installed - Azure TTS unavailable")
import hashlib
import ssl
import hmac
import base64
from datetime import datetime
from urllib.parse import urlencode
from typing import Optional, Dict, List, Any
import logging
import re
from collections import defaultdict
import weakref
from dataclasses import dataclass
import aiofiles
from concurrent.futures import ThreadPoolExecutor
import threading
from contextlib import asynccontextmanager
import uuid
import random
from aiohttp import ClientError

# æ–°å¢å¯¼å…¥
from knowledge_base import KnowledgeBase
from weather_service import WeatherService

# ===== SSL FIX - TEMPORARY =====
# Note: ssl and os are already imported above
os.environ['PYTHONHTTPSVERIFY'] = '0'

_original_create_default_context = ssl.create_default_context

def _create_unverified_context(*args, **kwargs):
    context = _original_create_default_context(*args, **kwargs)
    context.check_hostname = False
    context.verify_mode = ssl.CERT_NONE
    return context

ssl.create_default_context = _create_unverified_context
ssl._create_default_https_context = _create_unverified_context
# ===== END SSL FIX =====

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è¼‰å…¥ç’°å¢ƒè®Šé‡
load_dotenv()

# API Keys
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
TOGETHER_API_KEY = os.getenv('TOGETHER_API_KEY')
QWEN_API_KEY = os.getenv('QWEN_API_KEY')
XUNFEI_APP_ID = os.getenv('XUNFEI_APP_ID')
XUNFEI_API_KEY = os.getenv('XUNFEI_API_KEY')
XUNFEI_API_SECRET = os.getenv('XUNFEI_API_SECRET')
DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')

# Azure Cognitive Services (for Cantonese TTS)
AZURE_SPEECH_KEY = os.getenv('AZURE_SPEECH_KEY')
AZURE_SPEECH_REGION = os.getenv('AZURE_SPEECH_REGION')

# ===== LLM-CONTEXT-END: IMPORTS AND CONFIG =====

# ===== LLM-CONTEXT-START: API ENDPOINTS =====
# @LLM-CONTEXT: API ç«¯é»å®šç¾©
QWEN_API_URL = "https://dashscope-intl.aliyuncs.com/compatible-mode/v1/chat/completions"
# ===== LLM-CONTEXT-END: API ENDPOINTS =====

# èª¿è©¦æ—¥èªŒ
logger.debug(f"GEMINI_API_KEY: {'*' * len(GEMINI_API_KEY) if GEMINI_API_KEY else 'Not set'}")
logger.debug(f"TOGETHER_API_KEY: {'*' * len(TOGETHER_API_KEY) if TOGETHER_API_KEY else 'Not set'}")
logger.debug(f"QWEN_API_KEY: {'*' * len(QWEN_API_KEY) if QWEN_API_KEY else 'Not set'}")
logger.debug(f"XUNFEI_APP_ID: {XUNFEI_APP_ID if XUNFEI_APP_ID else 'Not set'}")
logger.debug(f"DEEPSEEK_API_KEY: {'*' * len(DEEPSEEK_API_KEY) if DEEPSEEK_API_KEY else 'Not set'}")

# HTML æ–‡ä»¶é…ç½®
HTML_FILE = os.getenv('HTML_FILE', 'index.html')
print(f"DEBUG: HTML_FILE = {HTML_FILE}")

# ===== LLM-CONTEXT-START: SYSTEM CONFIG =====
# @LLM-CONTEXT: ç³»çµ±é…ç½®
PERFORMANCE_CONFIG = {
    "MAX_CONNECTIONS_PER_VOICE": 3,
    "CONNECTION_IDLE_TIMEOUT": 480,  # 8åˆ†é˜
    "CACHE_MAX_SIZE": 500,
    "CACHE_TTL": 3600,  # 1å°æ™‚
    "PRELOAD_ENABLED": True,
    "CHUNK_SIZE": 2048,
    "FIRST_CHUNK_SIZE": 512,
    "MONITORING_ENABLED": True
}
# ===== LLM-CONTEXT-END: SYSTEM CONFIG =====


# ===== LLM-CONTEXT-START: MODEL CONFIG =====
# ===== Model Aliases ===== - unchanged
MODEL_ALIASES = {
    # Gemini: Frontend names -> Correct API names
    "gemini-2.5-flash": "gemini-1.5-flash-001",
    "gemini-2.0-flash": "gemini-1.5-flash-001",
    "gemini-1.5-flash": "gemini-1.5-flash-001",
    "gemini-pro": "gemini-1.5-pro-latest",
    "gemini-1.5-pro": "gemini-1.5-pro-latest",

    # TogetherAI: Frontend names -> å®Œæ•´çš„æ¨¡å‹ ID
    "together-deepseek": "deepseek-ai/DeepSeek-V3",
    "together-deepseek-coder": "deepseek-ai/deepseek-coder-33b-instruct",
    "together-llama-3.1-405b": "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
    "together-llama-3.1-70b": "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
    "together-llama-70b": "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",  # ç°¡çŸ­åˆ¥å
    "together-mixtral-8x7b": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "together-mixtral": "mistralai/Mixtral-8x7B-Instruct-v0.1",  # ç°¡çŸ­åˆ¥å
    "together-qwen-72b": "Qwen/Qwen2.5-72B-Instruct-Turbo",
    "together-qwen": "Qwen/Qwen2.5-72B-Instruct-Turbo",  # ç°¡çŸ­åˆ¥å
    
}

# Model Configurations - Updated with Qwen models
AVAILABLE_MODELS = {
    "qwen-turbo": {
        "name": "é€šç¾©åƒå• Turbo",
        "provider": "qwen",
        "model_id": "qwen-turbo-latest",
        "description": "å¿«é€ŸéŸ¿æ‡‰ï¼Œé©åˆä¸€èˆ¬å°è©±"
    },
    "qwen-plus": {
        "name": "é€šç¾©åƒå• Plus",
        "provider": "qwen", 
        "model_id": "qwen-plus-latest",
        "description": "æ›´å¼·å¤§çš„æ¨ç†èƒ½åŠ›"
    },
    # Together AI æ¨¡å‹
    "together-deepseek": {
        "name": "DeepSeek (Together)",
        "provider": "together",
        "model_id": "deepseek-ai/DeepSeek-V3",
        "description": "é€šé Together API ä½¿ç”¨ DeepSeek"
    },
    "together-llama-70b": {
        "name": "Llama 3.1 70B (Together)",
        "provider": "together",
        "model_id": "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
        "description": "Meta Llama 3.1 70B æ¨¡å‹"
    },
    "together-qwen": {
        "name": "Qwen 2.5 72B (Together)",
        "provider": "together",
        "model_id": "Qwen/Qwen2.5-72B-Instruct-Turbo",
        "description": "é€šç¾©åƒå• 2.5 72B æ¨¡å‹"
    },
    "together-mixtral": {
        "name": "Mixtral 8x7B (Together)",
        "provider": "together",
        "model_id": "mistralai/Mixtral-8x7B-Instruct-v0.1",
        "description": "Mistral AI Mixtral æ¨¡å‹ï¼ˆç²µèªæ”¯æ´æœ‰é™ï¼‰"
    }
}

# Edge TTS Voice Configurations - unchanged
EDGE_TTS_VOICES = {
    "zh-HK-HiuGaaiNeural": {
        "name": "æ›‰ä½³",
        "gender": "å¥³",
        "description": "é¦™æ¸¯ç²µèªå¥³è²ï¼Œè²éŸ³æ¸…æ™°è‡ªç„¶"
    },
    "zh-HK-WanLungNeural": {
        "name": "é›²é¾", 
        "gender": "ç”·",
        "description": "é¦™æ¸¯ç²µèªç”·è²ï¼Œè²éŸ³æ²‰ç©©"
    },
    "zh-HK-HiuMaanNeural": {
        "name": "æ›‰æ›¼",
        "gender": "å¥³", 
        "description": "é¦™æ¸¯ç²µèªå¥³è²ï¼Œè²éŸ³æº«æŸ”"
    }
}
# ===== LLM-CONTEXT-END: MODEL CONFIG =====

# ===== LLM-SKIP-START: KNOWLEDGE BASE =====
# @LLM-SKIP: çŸ¥è­˜åº«ç®¡ç† - ç©©å®šåŠŸèƒ½
# ===== çŸ¥è­˜åº«ç®¡ç† =====
# å·²ç§»é™¤æœ¬åœ° KnowledgeBase é¡ï¼Œè«‹ä½¿ç”¨ knowledge_base.py ä¸­çš„ç‰ˆæœ¬
# ===== LLM-SKIP-END: KNOWLEDGE BASE =====

# å…¨å±€æœåŠ¡å®ä¾‹
knowledge_base = KnowledgeBase()  # SQLiteç‰ˆæœ¬
weather_service = WeatherService()  # å¤©æ°”æœåŠ¡

# ===== TTSé€£æ¥æ± ç®¡ç† ===== - unchanged
@dataclass
class TTSConnection:
    voice: str
    status: str  # IDLE, ACTIVE, DEAD
    last_used: float
    connection_id: str
    communicate: Optional[edge_tts.Communicate] = None
    lock: Optional[asyncio.Lock] = None
    
    def __post_init__(self):
        if not self.lock:
            self.lock = asyncio.Lock()

# ===== LLM-SKIP-START: TTS ENGINE =====
# @LLM-SKIP: TTS é€£æ¥æ± å’Œç·©å­˜ - ç©©å®šåŠŸèƒ½ï¼ŒLLM ä¸éœ€è¦çœ‹
class TTSConnectionPool:
    def __init__(self):
        self.pools = defaultdict(list)
        self.locks = defaultdict(asyncio.Lock)
        self.max_connections = PERFORMANCE_CONFIG["MAX_CONNECTIONS_PER_VOICE"]
        self.idle_timeout = PERFORMANCE_CONFIG["CONNECTION_IDLE_TIMEOUT"]
        self.cleanup_task = None
        self._initialized = False
    
    async def _ensure_initialized(self):
        """ç¢ºä¿é€£æ¥æ± å·²åˆå§‹åŒ–"""
        if not self._initialized:
            await self._start_cleanup_task()
            self._initialized = True
    
    async def _start_cleanup_task(self):
        """å•Ÿå‹•é€£æ¥æ¸…ç†ä»»å‹™"""
        if self.cleanup_task is None:
            try:
                loop = asyncio.get_running_loop()
                self.cleanup_task = loop.create_task(self._periodic_cleanup())
            except RuntimeError:
                logger.warning("No running event loop, cleanup task will be started later")
    
    async def _periodic_cleanup(self):
        """å®šæœŸæ¸…ç†é–’ç½®é€£æ¥"""
        while True:
            try:
                await asyncio.sleep(60)  # æ¯åˆ†é˜æ¸…ç†ä¸€æ¬¡
                await self._cleanup_all_idle_connections()
            except Exception as e:
                logger.error(f"Cleanup task error: {e}")
    
    async def acquire(self, voice: str) -> TTSConnection:
        """ç²å–TTSé€£æ¥"""
        await self._ensure_initialized()
        
        async with self.locks[voice]:
            await self._clean_idle_connections(voice)
            
            for conn in self.pools[voice]:
                if conn.status == 'IDLE' and await self._probe_connection(conn):
                    conn.status = 'ACTIVE'
                    conn.last_used = time.time()
                    logger.debug(f"Reusing connection {conn.connection_id} for {voice}")
                    return conn
            
            if len(self.pools[voice]) < self.max_connections:
                new_conn = await self._create_connection(voice)
                self.pools[voice].append(new_conn)
                logger.info(f"Created new connection {new_conn.connection_id} for {voice}")
                return new_conn
            
            return await self._wait_for_connection(voice)
    
    async def release(self, connection: TTSConnection):
        """é‡‹æ”¾é€£æ¥"""
        async with self.locks[connection.voice]:
            if connection.status == 'ACTIVE':
                connection.status = 'IDLE'
                connection.last_used = time.time()
                logger.debug(f"Released connection {connection.connection_id}")
    
    async def _create_connection(self, voice: str) -> TTSConnection:
        """å‰µå»ºæ–°çš„TTSé€£æ¥"""
        connection_id = f"{voice}_{int(time.time() * 1000)}"
        conn = TTSConnection(
            voice=voice,
            status='ACTIVE',
            last_used=time.time(),
            connection_id=connection_id
        )
        
        await self._warmup_connection(conn)
        return conn
    
    async def _warmup_connection(self, conn: TTSConnection):
        """é ç†±é€£æ¥"""
        try:
            test_communicate = edge_tts.Communicate("æ¸¬è©¦", conn.voice)
            conn.communicate = test_communicate
            logger.debug(f"Warmed up connection {conn.connection_id}")
        except Exception as e:
            logger.error(f"Failed to warmup connection {conn.connection_id}: {e}")
            conn.status = 'DEAD'
    
    async def _probe_connection(self, conn: TTSConnection) -> bool:
        """æ¢æ¸¬é€£æ¥å¥åº·ç‹€æ…‹"""
        try:
            if conn.status == 'DEAD':
                return False
            
            if time.time() - conn.last_used > self.idle_timeout:
                conn.status = 'DEAD'
                return False
            
            return True
        except Exception as e:
            logger.warning(f"Connection probe failed for {conn.connection_id}: {e}")
            conn.status = 'DEAD'
            return False
    
    async def _clean_idle_connections(self, voice: str):
        """æ¸…ç†é–’ç½®é€£æ¥"""
        now = time.time()
        active_connections = []
        
        for conn in self.pools[voice]:
            if (conn.status == 'ACTIVE' or 
                (conn.status == 'IDLE' and now - conn.last_used < self.idle_timeout)):
                active_connections.append(conn)
            else:
                logger.info(f"Removing idle connection {conn.connection_id}")
        
        self.pools[voice] = active_connections
    
    async def _cleanup_all_idle_connections(self):
        """æ¸…ç†æ‰€æœ‰èªéŸ³çš„é–’ç½®é€£æ¥"""
        for voice in list(self.pools.keys()):
            async with self.locks[voice]:
                await self._clean_idle_connections(voice)
    
    async def _wait_for_connection(self, voice: str, timeout: int = 30) -> TTSConnection:
        """ç­‰å¾…é€£æ¥å¯ç”¨"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            for conn in self.pools[voice]:
                if conn.status == 'IDLE' and await self._probe_connection(conn):
                    conn.status = 'ACTIVE'
                    conn.last_used = time.time()
                    return conn
            
            await asyncio.sleep(0.1)
        
        raise TimeoutError(f"No available connections for voice {voice}")

# ===== æ™ºèƒ½ç·©å­˜ç³»çµ± ===== - unchanged
class IntelligentTTSCache:
    def __init__(self):
        self.cache = {}
        self.access_counts = defaultdict(int)
        self.last_access = {}
        self.cache_sizes = {}
        self.max_size = PERFORMANCE_CONFIG["CACHE_MAX_SIZE"]
        self.ttl = PERFORMANCE_CONFIG["CACHE_TTL"]
        self.lock = asyncio.Lock()
    
    def _generate_cache_key(self, text: str, voice: str, rate: int, pitch: int) -> str:
        """ç”Ÿæˆç·©å­˜éµ"""
        content = f"{text}|{voice}|{rate}|{pitch}"
        return hashlib.md5(content.encode()).hexdigest()
    
    async def get(self, text: str, voice: str, rate: int, pitch: int) -> Optional[bytes]:
        """ç²å–ç·©å­˜"""
        cache_key = self._generate_cache_key(text, voice, rate, pitch)

        async with self.lock:
            if cache_key in self.cache:
                if time.time() - self.last_access[cache_key] > self.ttl:
                    await self._remove(cache_key)
                    return None

                # âœ… Validate cached audio is not empty
                cached_data = self.cache[cache_key]
                if not cached_data or len(cached_data) == 0:
                    logger.warning(f"Found empty cached audio, removing: {cache_key[:8]}")
                    await self._remove(cache_key)
                    return None

                self.access_counts[cache_key] += 1
                self.last_access[cache_key] = time.time()

                logger.debug(f"Cache hit: {cache_key[:8]}...")
                return cached_data

        return None
    
    async def put(self, text: str, voice: str, rate: int, pitch: int, audio_data: bytes):
        """å­˜å…¥ç·©å­˜"""
        # âœ… Validate audio data is not empty before caching
        if not audio_data or len(audio_data) == 0:
            logger.warning(f"Refusing to cache empty audio for text: {text[:50]}")
            return

        cache_key = self._generate_cache_key(text, voice, rate, pitch)

        async with self.lock:
            if len(self.cache) >= self.max_size:
                await self._evict_lru()

            self.cache[cache_key] = audio_data
            self.cache_sizes[cache_key] = len(audio_data)
            self.access_counts[cache_key] = 1
            self.last_access[cache_key] = time.time()

            logger.debug(f"Cache put: {cache_key[:8]}... ({len(audio_data)} bytes)")
    
    async def _evict_lru(self):
        """æ·˜æ±°æœ€å°‘ä½¿ç”¨çš„ç·©å­˜é …"""
        if not self.cache:
            return
        
        lru_key = min(self.last_access.keys(), key=lambda k: self.last_access[k])
        await self._remove(lru_key)
        logger.debug(f"Evicted LRU cache: {lru_key[:8]}...")
    
    async def _remove(self, cache_key: str):
        """ç§»é™¤ç·©å­˜é …"""
        self.cache.pop(cache_key, None)
        self.access_counts.pop(cache_key, None)
        self.last_access.pop(cache_key, None)
        self.cache_sizes.pop(cache_key, None)
    
    def get_stats(self) -> dict:
        """ç²å–ç·©å­˜çµ±è¨ˆ"""
        total_size = sum(self.cache_sizes.values())
        return {
            "entries": len(self.cache),
            "total_size_mb": round(total_size / 1024 / 1024, 2),
            "hit_rate": sum(self.access_counts.values()) / max(len(self.cache), 1)
        }

# ===== æ€§èƒ½ç›£æ§ç³»çµ± ===== - unchanged
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            "first_chunk_latencies": [],
            "chunk_gaps": [],
            "error_counts": defaultdict(int),
            "request_counts": defaultdict(int),
            "cache_stats": {}
        }
        self.start_time = time.time()
    
    def record_first_chunk_latency(self, latency_ms: float):
        """è¨˜éŒ„é¦–å¹€å»¶é²"""
        self.metrics["first_chunk_latencies"].append(latency_ms)
        
        if len(self.metrics["first_chunk_latencies"]) > 1000:
            self.metrics["first_chunk_latencies"] = self.metrics["first_chunk_latencies"][-1000:]
    
    def record_chunk_gap(self, gap_ms: float):
        """è¨˜éŒ„å¥é–“é–“éš”"""
        self.metrics["chunk_gaps"].append(gap_ms)
        
        if len(self.metrics["chunk_gaps"]) > 1000:
            self.metrics["chunk_gaps"] = self.metrics["chunk_gaps"][-1000:]
    
    def record_error(self, error_type: str):
        """è¨˜éŒ„éŒ¯èª¤"""
        self.metrics["error_counts"][error_type] += 1
    
    def record_request(self, request_type: str):
        """è¨˜éŒ„è«‹æ±‚"""
        self.metrics["request_counts"][request_type] += 1
    
    def get_stats(self) -> dict:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        first_chunk_latencies = self.metrics["first_chunk_latencies"]
        chunk_gaps = self.metrics["chunk_gaps"]
        
        stats = {
            "uptime_seconds": int(time.time() - self.start_time),
            "total_requests": sum(self.metrics["request_counts"].values()),
            "total_errors": sum(self.metrics["error_counts"].values()),
            "error_rate": 0,
            "performance": {}
        }
        
        if stats["total_requests"] > 0:
            stats["error_rate"] = stats["total_errors"] / stats["total_requests"]
        
        if first_chunk_latencies:
            stats["performance"]["first_chunk"] = {
                "avg_ms": round(sum(first_chunk_latencies) / len(first_chunk_latencies), 2),
                "p95_ms": round(sorted(first_chunk_latencies)[int(len(first_chunk_latencies) * 0.95)], 2),
                "p99_ms": round(sorted(first_chunk_latencies)[int(len(first_chunk_latencies) * 0.99)], 2)
            }
        
        if chunk_gaps:
            stats["performance"]["chunk_gaps"] = {
                "avg_ms": round(sum(chunk_gaps) / len(chunk_gaps), 2),
                "p95_ms": round(sorted(chunk_gaps)[int(len(chunk_gaps) * 0.95)], 2)
            }
        
        return stats
# ===== LLM-SKIP-END: TTS ENGINE =====



# ===== æ–‡æœ¬é è™•ç†å„ªåŒ– ===== - unchanged
def optimize_text_for_cantonese_tts(text: str) -> str:
    """å„ªåŒ–æ–‡æœ¬ä»¥æ”¹å–„ç²µèªç™¼éŸ³å’Œæ¸›å°‘å»¶é²"""
    
    emoji_pattern = re.compile("["
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    
    emojis = emoji_pattern.findall(text)
    
    for i, emoji in enumerate(emojis):
        text = text.replace(emoji, f"__EMOJI_{i}__")
    
    quick_replacements = {
        r'\bAI\b': 'èª’æ„›',
        r'\bOK\b': 'okay',
        r'\bWiFi\b': 'æ­ªfai',
        r'\bUSB\b': 'U S B',
        r'\b2024\b': 'äºŒé›¶äºŒå››',
        r'\b2025\b': 'äºŒé›¶äºŒäº”',
        'å””ä¿‚': 'å”” ä¿‚',
        'å””å¥½': 'å”” å¥½',
        'å””çŸ¥': 'å”” çŸ¥',
    }
    
    for pattern, replacement in quick_replacements.items():
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    
    for i, emoji in enumerate(emojis):
        text = text.replace(f"__EMOJI_{i}__", emoji)
    
    return text

# ===== å…¨å±€å¯¦ä¾‹ ===== - unchanged
connection_pool = TTSConnectionPool()
tts_cache = IntelligentTTSCache()
performance_monitor = PerformanceMonitor()

# ===== Lifespan Context Manager ===== - unchanged
@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("æ­£åœ¨åˆå§‹åŒ–TTSé€£æ¥æ± ...")
    await connection_pool._ensure_initialized()
    logger.info("TTSé€£æ¥æ± åˆå§‹åŒ–å®Œæˆ")
    
    logger.info(f'{"="*70}')
    logger.info('ğŸš€ å°ç‹ç‹¸AIåŠ©æ‰‹ - æ¥µé€ŸTTSç‰ˆæœå‹™å™¨')
    logger.info(f'{"="*70}')
    logger.info(f'âš¡ æ€§èƒ½å„ªåŒ–åŠŸèƒ½:')
    logger.info(f'   ğŸ”— TTSé€£æ¥æ± : æœ€å¤§{PERFORMANCE_CONFIG["MAX_CONNECTIONS_PER_VOICE"]}é€£æ¥/èªéŸ³')
    logger.info(f'   ğŸ’¾ æ™ºèƒ½ç·©å­˜: æœ€å¤§{PERFORMANCE_CONFIG["CACHE_MAX_SIZE"]}é …')
    logger.info(f'   ğŸ“Š æ€§èƒ½ç›£æ§: {"å•Ÿç”¨" if PERFORMANCE_CONFIG["MONITORING_ENABLED"] else "ç¦ç”¨"}')
    logger.info(f'   ğŸš€ é åŠ è¼‰: {"å•Ÿç”¨" if PERFORMANCE_CONFIG["PRELOAD_ENABLED"] else "ç¦ç”¨"}')
    logger.info(f'   ğŸ“ éœæ…‹æ–‡ä»¶: {"å·²é…ç½®" if os.path.exists("static") else "æœªé…ç½®"}')
    logger.info(f'   ğŸ  ä¸»é é¢: {"å¯ç”¨" if os.path.exists(HTML_FILE) else "æœªæ‰¾åˆ°"}')
    logger.info(f'{"="*70}')
    
    yield
    
    logger.info("æ­£åœ¨æ¸…ç†TTSé€£æ¥æ± ...")
    if connection_pool.cleanup_task and not connection_pool.cleanup_task.done():
        connection_pool.cleanup_task.cancel()
        try:
            await connection_pool.cleanup_task
        except asyncio.CancelledError:
            pass
    logger.info("TTSé€£æ¥æ± æ¸…ç†å®Œæˆ")

# ===== å‰µå»º FastAPI å¯¦ä¾‹ ===== - unchanged
app = FastAPI(
    title="å°ç‹ç‹¸AIåŠ©æ‰‹ - æ¥µé€ŸTTSç‰ˆ", 
    version="2.0.0",
    lifespan=lifespan
)

# CORS é…ç½® - unchanged
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===== éœæ…‹æ–‡ä»¶æœå‹™é…ç½® ===== - unchanged
if not os.path.exists("static"):
    os.makedirs("static")
    logger.info("Created 'static' directory")

app.mount("/static", StaticFiles(directory="static"), name="static")

# ===== LLM-CONTEXT-START: MAIN API ROUTES =====
# @LLM-CONTEXT: ä¸»è¦ API è·¯ç”±
# æ·»åŠ æ ¹è·¯å¾‘è·¯ç”± - unchanged
@app.get("/")
async def root(request: Request):
    """æ ¹è·¯å¾‘ - è¿”å›ä¸»é é¢æˆ–APIè³‡è¨Š"""
    port = request.url.port or int(os.getenv('PORT', 5000))
    static_path = f'static/{HTML_FILE}'
    
    html_content = ""
    if os.path.exists(static_path):
        with open(static_path, "r", encoding="utf-8") as f:
            html_content = f.read()
    elif os.path.exists(HTML_FILE):
        with open(HTML_FILE, "r", encoding="utf-8") as f:
            html_content = f.read()
    
    if html_content:
        # Inject actual host/port/protocol to frontend
        host = request.url.hostname or "127.0.0.1"
        protocol = request.url.scheme or "http"
        
        # Create server config injection
        server_config = {
            "port": port,
            "host": host, 
            "protocol": protocol,
            "api_url": f"{protocol}://{host}:{port}"
        }
        
        injection = f'''<script>
window.SERVER_CONFIG = {json.dumps(server_config)};
console.log('ğŸ”Œ Server config injected:', window.SERVER_CONFIG);
</script>'''
        
        # Inject before </head>
        if "</head>" in html_content:
            html_content = html_content.replace("</head>", injection + "\n</head>")
        else:
            # Fallback: inject at beginning
            html_content = injection + "\n" + html_content
            
        return HTMLResponse(content=html_content)
    else:
        # If file not found, show detailed error
        return {
            "error": "HTML file not found",
            "searched_paths": [HTML_FILE, static_path],
            "current_dir": os.getcwd(),
            "files_in_current_dir": os.listdir('.'),
            "files_in_static": os.listdir('static') if os.path.exists('static') else []
        }

# ===== è«‹æ±‚æ¨¡å‹ ===== - unchanged
class TTSRequest(BaseModel):
    text: str
    voice: str = 'zh-HK-HiuGaaiNeural'
    rate: int = 160
    pitch: int = 100
    skip_browser: bool = False  # When true, skip browser TTS and use server fallback directly

class ChatRequest(BaseModel):
    prompt: str
    model: str = 'gemini-1.5-flash-001'
    responseLength: str = 'brief'

class QAPairRequest(BaseModel):
    category: str
    questions: List[str]
    answer: str

# ===== Log function =====
def log(message):
    print(f"[{time.strftime('%H:%M:%S')}] {message}")

# ===== API ç«¯é» ===== - unchanged except for qwen format check
@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ä¸¦è¿”å›æ€§èƒ½çµ±è¨ˆ"""
    cache_stats = tts_cache.get_stats()
    perf_stats = performance_monitor.get_stats()
    
    qwen_key_configured = bool(QWEN_API_KEY)
    qwen_key_valid_format = QWEN_API_KEY and QWEN_API_KEY.startswith('sk-') if QWEN_API_KEY else False
    
    return {
        "status": "healthy",
        "api_keys_configured": {
            "gemini": bool(GEMINI_API_KEY and GEMINI_API_KEY != 'your_gemini_api_key'),
            "together": bool(TOGETHER_API_KEY and TOGETHER_API_KEY != 'your_together_api_key'),  # é€™è£¡ä¿®æ”¹
            "qwen": {
                "configured": qwen_key_configured,
                "valid_format": qwen_key_valid_format
            },
            "xunfei": bool(XUNFEI_APP_ID and XUNFEI_API_KEY and XUNFEI_API_SECRET),
            "deepseek": bool(DEEPSEEK_API_KEY)
        },
        "performance": perf_stats,
        "cache": cache_stats,
        "pool_status": {
            voice: len(connections) for voice, connections in connection_pool.pools.items()
        }
    }

@app.get("/models")
async def get_models():
    return {"models": AVAILABLE_MODELS}

@app.get("/voices")
async def get_voices():
    return {"voices": EDGE_TTS_VOICES}

@app.post("/api/tts/stream")
async def tts_stream_optimized(req: TTSRequest):
    """å„ªåŒ–ç‰ˆ TTS æµå¼åˆæˆï¼šå…ˆåšå»£æ±è©±æ•¸å­—/å–®ä½ã€é»ã€è®€æ³•é è™•ç†ï¼Œå†åˆæˆ"""
    start_time = time.time()
    performance_monitor.record_request("tts_stream")

    try:
        # Validate input
        if not req.text or not req.text.strip():
            logger.warning("TTS request with empty text")
            raise HTTPException(status_code=400, detail="Text cannot be empty")

        # Log the request
        logger.info(f"TTS request: voice={req.voice}, rate={req.rate}, pitch={req.pitch}, text_length={len(req.text)}")

        # 1) å…ˆæŸ¥å¿«å–
        cached_audio = await tts_cache.get(req.text, req.voice, req.rate, req.pitch)
        if cached_audio:
            logger.info(f"TTS cache hit: {req.text[:30]}...")
            return await _stream_cached_audio(cached_audio, start_time)

        # 2) æ–‡å­—æ¸…æ´— + å»£æ±è©±é è™•ç†ï¼ˆé—œéµï¼šé€™è£¡æœƒæŠŠ 32.5Â°C è½‰æˆã€æ”æ°32é»5åº¦ã€ï¼‰
        base_text = strip_html_tags(req.text)
        processed_text = preprocess_for_cantonese_tts(
            optimize_text_for_cantonese_tts(base_text)
        )

        # Validate processed text
        if not processed_text or not processed_text.strip():
            logger.warning(f"Processed text is empty after preprocessing. Original: {req.text[:50]}")
            raise HTTPException(status_code=400, detail="Processed text is empty")

        # æ–¹ä¾¿ä½ åœ¨ console ç›´æ¥çœ‹åˆ°æ˜¯å¦å·²ç¶“åŒ…å«ã€Œé»ã€
        logger.info(f"[TTS preprocessed] {processed_text[:120]}")

        # 3) å˜—è©¦ Edge TTS (å„ªå…ˆ)
        edge_tts_failed = False
        edge_error_msg = ""
        connection = None

        try:
            connection = await connection_pool.acquire(req.voice)
            # 4) åˆæˆä¸¦ä»¥ä¸²æµæ–¹å¼å›å‚³
            return await _synthesize_and_stream(connection, processed_text, req, start_time)
        except Exception as edge_error:
            edge_tts_failed = True
            edge_error_msg = str(edge_error)
            performance_monitor.record_error("edge_tts_failure")
            logger.warning(f"âš ï¸ Edge TTS failed: {edge_error}")
        finally:
            # 5) ä¸€å®šè¦é‡‹æ”¾é€£ç·š
            if connection:
                await connection_pool.release(connection)

        # 6) å¦‚æœ Edge TTS å¤±æ•—
        if edge_tts_failed:
            # If skip_browser=False, tell client to try browser TTS first (priority 2)
            if not req.skip_browser:
                logger.info("ğŸŒ Edge TTS failed, instructing client to try Browser TTS (Danny)")
                raise HTTPException(
                    status_code=503,
                    detail="Edge TTS unavailable. Try browser TTS first.",
                    headers={"X-TTS-Fallback": "browser"}
                )

            # If skip_browser=True, try Azure TTS (priority 3)
            logger.info("ğŸ”„ Attempting fallback to Azure TTS (Cantonese)...")
            try:
                return await _synthesize_with_azure(processed_text, req, start_time)
            except Exception as azure_error:
                performance_monitor.record_error("azure_tts_failure")
                logger.warning(f"âš ï¸ Azure TTS also failed: {azure_error}")
                logger.info("ğŸ”„ Attempting fallback to gTTS (Mandarin)...")

                # 7) å¦‚æœ Azure ä¹Ÿå¤±æ•—ï¼Œå˜—è©¦ gTTS (Mandarin) (priority 4)
                try:
                    return await _synthesize_with_gtts(processed_text, req, start_time)
                except Exception as gtts_error:
                    performance_monitor.record_error("gtts_failure")
                    logger.error(f"âŒ All TTS providers failed")
                    # All TTS providers failed
                    raise HTTPException(
                        status_code=500,
                        detail=f"All TTS providers failed. Edge: {edge_error_msg[:50]}, Azure: {str(azure_error)[:50]}, gTTS: {str(gtts_error)[:50]}"
                    )

    except HTTPException:
        raise  # Re-raise HTTP exceptions as-is
    except Exception as e:
        performance_monitor.record_error("tts_synthesis")
        logger.exception(f"TTS error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===== LLM-CONTEXT-END: MAIN API ROUTES =====

# ===== WebSocket Endpoint =====
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    logger.info("WebSocket client connected")
    try:
        while True:
            # Keep the connection alive and wait for client to disconnect
            await websocket.receive_text()
    except WebSocketDisconnect:
        logger.info("WebSocket client disconnected")
    except Exception as e:
        logger.error(f"WebSocket error: {e}")


async def _stream_cached_audio(cached_audio: bytes, start_time: float):
    """æµå¼è¿”å›ç·©å­˜éŸ³é » - Fixed with proper headers"""
    # âœ… Validate cached audio is not empty
    if not cached_audio or len(cached_audio) == 0:
        logger.error("Cannot stream empty cached audio!")
        raise HTTPException(status_code=500, detail="Cached audio is empty")

    first_chunk_size = PERFORMANCE_CONFIG["FIRST_CHUNK_SIZE"]
    chunk_size = PERFORMANCE_CONFIG["CHUNK_SIZE"]

    async def audio_generator():
        try:
            # Send first chunk
            yield cached_audio[:first_chunk_size]

            first_chunk_latency = (time.time() - start_time) * 1000
            performance_monitor.record_first_chunk_latency(first_chunk_latency)

            # Stream remaining chunks
            for i in range(first_chunk_size, len(cached_audio), chunk_size):
                yield cached_audio[i:i + chunk_size]
                await asyncio.sleep(0.001)
        except Exception as e:
            logger.error(f"Error streaming cached audio: {e}")
            # Don't raise, just complete the stream

    # Proper headers for chunked encoding
    headers = {
        "Cache-Control": "public, max-age=3600",
        "Connection": "keep-alive",
        "X-Content-Type-Options": "nosniff",
        "Content-Length": str(len(cached_audio))
    }

    return StreamingResponse(
        audio_generator(),
        media_type="audio/mpeg",
        headers=headers
    )


async def _synthesize_with_gtts(text: str, req: TTSRequest, start_time: float):
    """Synthesize using Google TTS (gTTS) as fallback"""
    logger.info(f"ğŸ”„ Attempting gTTS synthesis: text_len={len(text)}, text='{text[:80]}...'")

    try:
        # Map voice to language (gTTS doesn't support voice selection)
        # Use zh-TW (Taiwan/Mandarin) as closest to Cantonese
        lang = 'zh-TW' if 'HK' in req.voice else 'zh-CN'

        # Create gTTS object
        tts = gTTS(text=text, lang=lang, slow=False)

        # Generate audio to BytesIO
        audio_fp = io.BytesIO()
        # Use run_in_executor for Python 3.8 compatibility (to_thread added in 3.9)
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, tts.write_to_fp, audio_fp)
        audio_fp.seek(0)

        audio_data = audio_fp.read()

        if not audio_data or len(audio_data) == 0:
            raise Exception("gTTS returned empty audio")

        logger.info(f"âœ… gTTS synthesis success: {len(audio_data)} bytes")

        # Cache the audio
        try:
            await tts_cache.put(req.text, req.voice, req.rate, req.pitch, audio_data)
        except Exception as cache_error:
            logger.warning(f"Failed to cache gTTS audio: {cache_error}")

        # Stream the audio
        async def audio_generator():
            chunk_size = PERFORMANCE_CONFIG["CHUNK_SIZE"]
            for i in range(0, len(audio_data), chunk_size):
                yield audio_data[i:i + chunk_size]
                await asyncio.sleep(0.001)

        headers = {
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Content-Type-Options": "nosniff"
        }

        return StreamingResponse(
            audio_generator(),
            media_type="audio/mpeg",
            headers=headers
        )

    except Exception as e:
        logger.error(f"âŒ gTTS synthesis failed: {e}", exc_info=True)
        raise


async def _synthesize_with_azure(text: str, req: TTSRequest, start_time: float):
    """Synthesize using Azure Cognitive Services TTS (Cantonese)"""
    if not AZURE_TTS_AVAILABLE:
        raise Exception("Azure Speech SDK not installed")

    if not AZURE_SPEECH_KEY or not AZURE_SPEECH_REGION:
        raise Exception("Azure credentials not configured in .env")

    logger.info(f"ğŸ”„ Attempting Azure TTS synthesis: text_len={len(text)}, text='{text[:80]}...'")

    try:
        # Configure speech service
        speech_config = speechsdk.SpeechConfig(
            subscription=AZURE_SPEECH_KEY,
            region=AZURE_SPEECH_REGION
        )

        # Set voice based on requested voice
        if "HiuGaai" in req.voice or "hiugaai" in req.voice.lower():
            voice_name = "zh-HK-HiuGaaiNeural"  # Female Cantonese
        elif "HiuMaan" in req.voice or "hiumaan" in req.voice.lower():
            voice_name = "zh-HK-HiuMaanNeural"  # Female Cantonese
        elif "WanLung" in req.voice or "wanlung" in req.voice.lower():
            voice_name = "zh-HK-WanLungNeural"  # Male Cantonese
        else:
            voice_name = "zh-HK-HiuGaaiNeural"  # Default to HiuGaai

        speech_config.speech_synthesis_voice_name = voice_name

        # Build SSML with rate and pitch control
        # Convert rate (default 160 = 160%) to SSML format
        # Convert pitch (default 100 = 100% = no change) to relative percentage
        rate_percent = req.rate  # Already a percentage (e.g., 160 = 160%)
        pitch_offset = req.pitch - 100  # 100 is baseline, so pitch=120 -> +20%

        # Escape XML special characters in text
        import html
        escaped_text = html.escape(text)

        ssml = f'''<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="zh-HK">
    <voice name="{voice_name}">
        <prosody rate="{rate_percent}%" pitch="{pitch_offset:+d}%">
            {escaped_text}
        </prosody>
    </voice>
</speak>'''

        logger.debug(f"ğŸ”Š Azure TTS SSML: rate={rate_percent}%, pitch={pitch_offset:+d}%")

        # Configure to synthesize to default output (internal buffer)
        # We'll extract audio_data from the result
        synthesizer = speechsdk.SpeechSynthesizer(
            speech_config=speech_config,
            audio_config=None  # None = synthesize to internal buffer
        )

        # Synthesize using SSML (run in thread pool to not block event loop)
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            lambda: synthesizer.speak_ssml_async(ssml).get()
        )

        # Check result
        if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
            audio_data = result.audio_data

            if not audio_data or len(audio_data) == 0:
                raise Exception("Azure TTS returned empty audio")

            logger.info(f"âœ… Azure TTS synthesis success: {len(audio_data)} bytes, voice={voice_name}")

            # Cache the audio
            try:
                await tts_cache.put(req.text, req.voice, req.rate, req.pitch, audio_data)
            except Exception as cache_error:
                logger.warning(f"Failed to cache Azure TTS audio: {cache_error}")

            # Stream the audio
            async def audio_generator():
                chunk_size = PERFORMANCE_CONFIG["CHUNK_SIZE"]
                for i in range(0, len(audio_data), chunk_size):
                    yield audio_data[i:i + chunk_size]
                    await asyncio.sleep(0.001)

            headers = {
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Content-Type-Options": "nosniff"
            }

            return StreamingResponse(
                audio_generator(),
                media_type="audio/mpeg",
                headers=headers
            )

        elif result.reason == speechsdk.ResultReason.Canceled:
            cancellation = result.cancellation_details
            error_msg = f"Azure TTS canceled: {cancellation.reason}"
            if cancellation.reason == speechsdk.CancellationReason.Error:
                error_msg += f" - {cancellation.error_details}"
            logger.error(error_msg)
            raise Exception(error_msg)

        else:
            raise Exception(f"Azure TTS unexpected result: {result.reason}")

    except Exception as e:
        logger.error(f"âŒ Azure TTS synthesis failed: {e}", exc_info=True)
        raise


async def _synthesize_and_stream(connection: TTSConnection, text: str, req: TTSRequest, start_time: float):
    """åˆæˆä¸¦æµå¼è¿”å›éŸ³é » - Fixed with proper error handling and stream completion"""
    # âœ… Validate text is not empty before synthesis
    if not text or not text.strip():
        logger.error(f"Cannot synthesize empty text! Original request text: {req.text[:100]}")
        raise HTTPException(status_code=400, detail="Text for synthesis is empty")

    rate_str = f"{req.rate - 100:+d}%"
    pitch_str = f"{req.pitch - 100:+d}Hz"

    logger.info(f"TTS synthesis starting: voice={req.voice}, rate={rate_str}, pitch={pitch_str}, text_len={len(text)}, text='{text[:80]}...'")

    # âš ï¸ Test Edge TTS first before streaming - if it fails immediately, raise exception for fallback
    communicate = edge_tts.Communicate(text, req.voice, rate=rate_str, pitch=pitch_str)

    # Try to get the first chunk to verify Edge TTS is working
    try:
        stream_iterator = communicate.stream()
        first_chunk = await stream_iterator.__anext__()
        # If we got here, Edge TTS is working, proceed with normal streaming
    except Exception as test_error:
        logger.error(f"Edge TTS failed on first chunk: {test_error}")
        raise  # Raise exception to trigger gTTS fallback

    async def audio_generator():
        audio_buffer = io.BytesIO()
        first_chunk_sent = False
        chunk_count = 0
        has_audio = False  # Track if any audio was generated

        try:
            # Process the first chunk we already fetched
            if first_chunk and first_chunk["type"] == "audio":
                audio_data = first_chunk["data"]
                audio_buffer.write(audio_data)
                has_audio = True
                first_chunk_sent = True
                first_chunk_size = min(PERFORMANCE_CONFIG["FIRST_CHUNK_SIZE"], len(audio_data))
                logger.debug(f"Sending first chunk: {first_chunk_size} bytes")
                yield audio_data[:first_chunk_size]
                first_chunk_latency = (time.time() - start_time) * 1000
                performance_monitor.record_first_chunk_latency(first_chunk_latency)
                if len(audio_data) > first_chunk_size:
                    yield audio_data[first_chunk_size:]
                chunk_count += 1

            # Continue with remaining chunks
            async for chunk in stream_iterator:
                if chunk["type"] == "audio":
                    audio_data = chunk["data"]
                    audio_buffer.write(audio_data)
                    has_audio = True

                    # Just yield the data directly since first chunk already handled
                    yield audio_data
                    chunk_count += 1

                    if chunk_count % 5 == 0:
                        await asyncio.sleep(0.001)

            # Log completion status
            logger.info(f"TTS synthesis completed: {chunk_count} chunks, {len(audio_buffer.getvalue())} bytes, has_audio={has_audio}")

        except (ClientError, OSError) as exc:
            performance_monitor.record_error("tts_network_unreachable")
            logger.error(f"Edge TTS network error for text '{text[:100]}...': {exc}")
            # Cannot raise HTTPException here - response already started streaming
            # Client will receive empty/partial audio and handle with fallback

        except Exception as exc:
            performance_monitor.record_error("tts_stream_failure")
            logger.error(f"Edge TTS streaming error for text '{text[:100]}...': {exc}", exc_info=True)
            # Cannot raise HTTPException here - response already started streaming
            # Client will receive empty/partial audio and handle with fallback
        finally:
            # Always try to cache the audio we did generate
            complete_audio = audio_buffer.getvalue()
            if len(complete_audio) > 0:
                try:
                    await tts_cache.put(req.text, req.voice, req.rate, req.pitch, complete_audio)
                    logger.debug(f"Cached TTS audio: {req.text[:30]}... ({len(complete_audio)} bytes)")
                except Exception as cache_error:
                    logger.warning(f"Failed to cache TTS audio: {cache_error}")
            else:
                # âœ… Enhanced logging when no audio generated
                logger.error(f"âš ï¸ TTS synthesis generated NO audio data! Text: '{text}' | Original: '{req.text}' | Voice: {req.voice} | Rate: {rate_str} | Pitch: {pitch_str}")

    # Proper headers for chunked encoding
    headers = {
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Content-Type-Options": "nosniff"
    }

    return StreamingResponse(
        audio_generator(),
        media_type="audio/mpeg",
        headers=headers
    )

@app.post("/api/tts")
async def tts_alias(req: TTSRequest):
    """TTSåˆ¥åç«¯é»ï¼ˆå‘å¾Œå…¼å®¹ï¼‰- unchanged"""
    return await tts_stream_optimized(req)

@app.post("/api/telemetry")
async def record_telemetry(request: Request):
    """è¨˜éŒ„é™æ¸¬æ•¸æ“š - unchanged"""
    try:
        data = await request.json()
        telemetry_type = data.get("type")
        
        if telemetry_type == "first_chunk":
            latency = data.get("data", 0)
            performance_monitor.record_first_chunk_latency(latency)
        elif telemetry_type == "chunk_gap":
            gaps = data.get("data", [])
            for gap in gaps[-5:]:
                performance_monitor.record_chunk_gap(gap)
        elif telemetry_type == "error":
            performance_monitor.record_error("client_error")
        
        return {"status": "recorded"}
    except Exception as e:
        logger.error(f"Telemetry error: {e}")
        return {"status": "error", "message": str(e)}

@app.get("/api/performance")
async def get_performance_metrics():
    """ç²å–æ€§èƒ½æŒ‡æ¨™ - unchanged"""
    return {
        "performance": performance_monitor.get_stats(),
        "cache": tts_cache.get_stats(),
        "pool_status": {
            voice: {
                "total": len(connections),
                "active": sum(1 for c in connections if c.status == 'ACTIVE'),
                "idle": sum(1 for c in connections if c.status == 'IDLE')
            }
            for voice, connections in connection_pool.pools.items()
        },
        "config": PERFORMANCE_CONFIG
    }

@app.get("/api/test-qwen")
async def test_qwen_api():
    """æ¸¬è©¦ Qwen API é€£æ¥ - unchanged"""
    if not QWEN_API_KEY:
        return {"status": "error", "message": "QWEN_API_KEY is not configured"}


# @LLM-CONTEXT: API ç«¯é»å®šç¾©    
    QWEN_API_URL = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"


    request_body = {
        "model": "qwen-plus",
        "input": {
            "messages": [
                {"role": "user", "content": "ä½ å¥½"}
            ]
        },
        "parameters": {
            "stream": False,
            "temperature": 0.7,
            "max_tokens": 50
        }
    }
    
    async with httpx.AsyncClient(timeout=30.0) as client:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {QWEN_API_KEY}"
        }
        
        try:
            response = await client.post(QWEN_API_URL, json=request_body, headers=headers)
            response_data = response.json()
            
            if response.status_code == 200:
                return {
                    "status": "success",
                    "message": "Qwen API connection successful",
                    "response": response_data
                }
            else:
                return {
                    "status": "error",
                    "message": f"Qwen API returned status code {response.status_code}",
                    "response": response_data
                }
        except Exception as e:
            return {
                "status": "error",
                "message": f"Failed to connect to Qwen API: {str(e)}"
            }

# ===== çŸ¥è­˜åº« API =====
@app.get("/api/knowledge/qa-pairs")
async def get_qa_pairs():
    """ç²å–æ‰€æœ‰å•ç­”å°"""
    try:
        qa_pairs = knowledge_base.get_all_qa_pairs()
        return {
            "status": "success",
            "data": qa_pairs
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/knowledge/qa-pairs")
async def add_qa_pair_endpoint(req: QAPairRequest):
    """æ·»åŠ æ–°çš„å•ç­”å°"""
    try:
        qa_pair = knowledge_base.add_qa_pair(req.questions[0], req.answer)  # Simplified for now
        return {
            "status": "success",
            "data": qa_pair
        }
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/knowledge/qa-pairs/{qa_id}/toggle")
async def toggle_qa_pair(qa_id: int):
    """å•Ÿç”¨/åœç”¨å•ç­”å°"""
    try:
        result = knowledge_base.toggle_qa_pair(qa_id)
        if result:
            return {"status": "success", "data": result}
        else:
            raise HTTPException(status_code=404, detail="QA pair not found")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/knowledge/qa-pairs/{qa_id}")
async def delete_qa_pair(qa_id: int):
    """åˆªé™¤å•ç­”å°"""
    try:
        success = knowledge_base.delete_qa_pair(qa_id)
        if success:
            return {"status": "success"}
        else:
            raise HTTPException(status_code=404, detail="QA pair not found")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/knowledge/qa-pairs/{qa_id}")
async def update_qa_pair(qa_id: int, req: QAPairRequest):
    """æ›´æ–°å•ç­”å°"""
    try:
        result = knowledge_base.update_qa_pair(qa_id, req.questions[0], req.answer)
        if result:
            return {"status": "success", "data": result}
        else:
            raise HTTPException(status_code=404, detail="QA pair not found")
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ===== LLM-CONTEXT-START: CHAT ROUTE =====
# @LLM-CONTEXT: æ ¸å¿ƒèŠå¤©è·¯ç”±é‚è¼¯
# ===== Chat API =====
@app.post("/api/chat")
async def chat(req: ChatRequest):
    """èŠå¤©APIï¼ˆæ·»åŠ æ€§èƒ½ç›£æ§ï¼‰"""
    performance_monitor.record_request("chat")
    
    # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯å¤©æ°”æŸ¥è¯¢
    weather_intent = weather_service.extract_weather_intent(req.prompt)
    if weather_intent:
        logger.info(f"Weather query detected: {req.prompt[:30]}...")
        weather_data = await weather_service.get_weather(weather_intent['date'])
        if weather_data:
            response = weather_service.format_weather_response(weather_data)
            
            # è¿”å›å¤©æ°”å“åº”
            async def weather_event_generator():
                response_data = {
                    'choices': [{
                        'delta': {'content': response},
                        'finish_reason': None
                    }]
                }
                yield f"data: {json.dumps(response_data, ensure_ascii=False)}\n\n"
                yield f"data: [DONE]\n\n"
            
            return StreamingResponse(weather_event_generator(), media_type="text/event-stream")
    
    # å†æ£€æŸ¥çŸ¥è¯†åº“
    kb_answer = knowledge_base.find_answer(req.prompt)
    if kb_answer:
        logger.info(f"Knowledge base hit: {req.prompt[:30]}...")
        
        # è¿”å›çŸ¥è¯†åº“ç­”æ¡ˆçš„æµå¼å“åº”
        async def kb_event_generator():
            response = {
                'choices': [{
                    'delta': {'content': kb_answer},
                    'finish_reason': None
                }]
            }
            yield f"data: {json.dumps(response, ensure_ascii=False)}\n\n"
            yield f"data: [DONE]\n\n"
        
        return StreamingResponse(kb_event_generator(), media_type="text/event-stream")
    
    # å¦‚æœéƒ½æ²¡æœ‰åŒ¹é…ï¼Œä½¿ç”¨ AI æ¨¡å‹å¤„ç†
    model_config = AVAILABLE_MODELS.get(req.model)
    
    if model_config:
        provider = model_config.get('provider')
        
        if provider == 'together':
            return await handle_together_request(req.prompt, model_config, req.responseLength)
        elif provider == 'qwen':
            return await chat_with_qwen(req)
        elif provider == 'deepseek':
            return await chat_with_deepseek(req)
        else:
            raise HTTPException(status_code=501, detail=f"Provider {provider} is not implemented")
    
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨åˆ«å
    model_name = MODEL_ALIASES.get(req.model, req.model)
    
    try:
        if model_name.startswith('deepseek'):
            return await chat_with_deepseek(req)
        elif req.model.startswith('together'):
            # å¯¹äº together æ¨¡å‹ï¼Œå°è¯•ä»åˆ«åè·å–å®Œæ•´çš„ model_id
            full_model_id = MODEL_ALIASES.get(req.model)
            if not full_model_id:
                raise HTTPException(status_code=400, detail=f"Unknown Together model: {req.model}")
            
            model_config = {
                "provider": "together",
                "model_id": full_model_id,
                "name": req.model
            }
            return await handle_together_request(req.prompt, model_config, req.responseLength)
        elif model_name.startswith('qwen'):
            return await chat_with_qwen(req)
        elif model_name.startswith('xunfei'):
            return await chat_with_xunfei(req)
        else:
            req.model = model_name
            return await chat_with_gemini(req)
            
    except Exception as e:
        performance_monitor.record_error("chat_general")
        raise HTTPException(status_code=500, detail=str(e))
# ===== LLM-CONTEXT-END: CHAT ROUTE =====


# ===== LLM-REF-START: CHAT HANDLERS =====
# @LLM-REF: èŠå¤©è™•ç†å‡½æ•¸ - å·²ç©©å®šé‹è¡Œ
async def chat_with_deepseek(req: ChatRequest):
    """ä½¿ç”¨ DeepSeek API é€²è¡ŒèŠå¤© - unchanged"""
    if not DEEPSEEK_API_KEY:
        raise HTTPException(status_code=500, detail="DEEPSEEK_API_KEY is not configured")
    
    DEEPSEEK_API_URL = "https://api.deepseek.com/chat/completions"
    
    if req.responseLength == 'brief':
        system_prompt = "ä½ ä¿‚ä¸€å€‹å°ˆç‚ºé¦™æ¸¯å°å­¸ç”Ÿè¨­è¨ˆå˜…AIåŠ©æ‰‹ã€‚è«‹ç”¨åœ°é“å»£æ±è©±å›ç­”ã€‚ç”¨2-3å¥è©±ç°¡æ½”å›ç­”ï¼Œä¸è¶…é50å€‹å­—ã€‚å¯ä»¥é©ç•¶ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿä»¤å°è©±æ›´ç”Ÿå‹•æœ‰è¶£ã€‚ğŸ˜Š"
    else:
        system_prompt = "ä½ ä¿‚ä¸€å€‹å°ˆç‚ºé¦™æ¸¯å°å­¸ç”Ÿè¨­è¨ˆå˜…AIåŠ©æ‰‹ã€‚è«‹ç”¨åœ°é“å»£æ±è©±å›ç­”ã€‚è©³ç´°è§£é‡‹ï¼Œç”¨ä¾‹å­èªªæ˜ï¼Œä»¤å°æœ‹å‹å®¹æ˜“æ˜ç™½ã€‚å¯ä»¥é©ç•¶ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿã€‚"
    
    request_body = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": req.prompt}
        ],
        "stream": True,
        "temperature": 0.7,
        "max_tokens": 200 if req.responseLength == 'detailed' else 100
    }
    
    async def event_generator():
        async with httpx.AsyncClient(timeout=60.0) as client:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
            }
            
            try:
                async with client.stream("POST", DEEPSEEK_API_URL, json=request_body, headers=headers) as response:
                    if response.status_code != 200:
                        performance_monitor.record_error("deepseek_api")
                        error_body = await response.aread()
                        error_text = error_body.decode('utf-8')
                        
                        error_response = {
                            'choices': [{
                                'delta': {'content': f'DeepSeek API éŒ¯èª¤ ({response.status_code}): {error_text[:200]}'},
                                'finish_reason': 'error'
                            }]
                        }
                        yield f"data: {json.dumps(error_response, ensure_ascii=False)}\n\n"
                        yield f"data: [DONE]\n\n"
                        return
                    
                    buffer = ""
                    async for chunk in response.aiter_text():
                        buffer += chunk
                        lines = buffer.split('\n')
                        buffer = lines[-1]
                        
                        for line in lines[:-1]:
                            line = line.strip()
                            if not line:
                                continue
                            
                            if line.startswith("data: "):
                                data_str = line[6:]
                                
                                if data_str == "[DONE]":
                                    yield f"data: [DONE]\n\n"
                                else:
                                    try:
                                        yield f"{line}\n\n"
                                    except Exception:
                                        continue
                                        
            except Exception as e:
                performance_monitor.record_error("deepseek_stream")
                error_response = {'choices': [{'delta': {'content': f'DeepSeek æœå‹™éŒ¯èª¤: {str(e)}'}, 'finish_reason': 'error'}]}
                yield f"data: {json.dumps(error_response, ensure_ascii=False)}\n\n"
                yield f"data: [DONE]\n\n"
    
    return StreamingResponse(event_generator(), media_type="text/event-stream")

def clean_together_output(text: str, model_id: str) -> str:
    """æ¸…ç† Together API æ¨¡å‹çš„è¼¸å‡º"""
    
    # ç§»é™¤ "Note:" ä¹‹å¾Œçš„æ‰€æœ‰å…§å®¹
    if "Note:" in text:
        text = text.split("Note:")[0].strip()
    
    # ç§»é™¤å¸¸è¦‹çš„ç³»çµ±è¨»é‡‹æ¨¡å¼
    patterns_to_remove = [
        r'\[.*?\]',  # ç§»é™¤æ–¹æ‹¬è™Ÿå…§çš„å…§å®¹
        r'\(Note:.*?\)',  # ç§»é™¤æ‹¬è™Ÿå…§çš„ Note
        r'<.*?>',  # ç§»é™¤ HTML æ¨™ç±¤
        r'Note:.*$',  # ç§»é™¤ Note: åˆ°çµå°¾çš„å…§å®¹
        r'PS:.*$',  # ç§»é™¤ PS: åˆ°çµå°¾çš„å…§å®¹
        r'P\.S\..*$',  # ç§»é™¤ P.S. åˆ°çµå°¾çš„å…§å®¹
    ]
    
    for pattern in patterns_to_remove:
        text = re.sub(pattern, '', text, flags=re.MULTILINE | re.DOTALL)
    
    # æ¸…ç†äº‚ç¢¼å’Œç‰¹æ®Šå­—ç¬¦
    text = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')
    
    # ç§»é™¤å¤šé¤˜çš„ç©ºç™½
    text = ' '.join(text.split())
    
    return text.strip()

async def handle_together_request(prompt: str, model_config: dict, response_length: str):
    log(f"Calling Together API - Model: {model_config['model_id']}, Length: {response_length}")
    
    if not TOGETHER_API_KEY or TOGETHER_API_KEY == 'your_together_api_key':
        raise HTTPException(status_code=500, detail="TOGETHER_API_KEY is not configured")
    
    TOGETHER_API_URL = "https://api.together.xyz/v1/chat/completions"
    
    length_configs = {
        'very_brief': {'max_tokens': 50, 'extra_prompt': 'ç”¨1å¥è©±å›ç­”ï¼Œä¸è¶…é20å€‹å­—ã€‚'},
        'brief': {'max_tokens': 100, 'extra_prompt': 'ç”¨2-3å¥è©±ç°¡æ½”å›ç­”ï¼Œä¸è¶…é50å€‹å­—ã€‚'},
        'normal': {'max_tokens': 200, 'extra_prompt': 'ç”¨4-5å¥è©±å›ç­”ï¼Œä¸è¶…é100å€‹å­—ã€‚'},
        'detailed': {'max_tokens': 350, 'extra_prompt': 'è©³ç´°è§£é‡‹ï¼Œå¯ä»¥åŒ…å«ä¾‹å­ã€‚'}
    }
    config = length_configs.get(response_length, length_configs['normal'])
    
    # ç‚º Mixtral æ¨¡å‹ä½¿ç”¨æ›´æ˜ç¢ºçš„æŒ‡ä»¤
    if "mixtral" in model_config['model_id'].lower():
        system_prompt = f"""You are an AI assistant designed for Hong Kong primary school students. 
IMPORTANT RULES:
1. Reply ONLY in Cantonese (å»£æ±è©±)
2. {config['extra_prompt']}
3. Do NOT use emojis or special characters
4. Do NOT add any notes, explanations, or meta-commentary about your response
5. Be friendly and use simple language
6. Give direct answers without any system messages

Example good response: ä½ å¥½ï¼æˆ‘ä¿‚ä½ å˜…AIåŠ©æ‰‹ï¼Œå¯ä»¥å¹«ä½ è§£ç­”å•é¡Œã€‚
Example bad response: ä½ å¥½ï¼ğŸ˜Š [Note: Using friendly tone] I'm here to help."""
    else:
        system_prompt = f"""ä½ ä¿‚ä¸€å€‹å°ˆç‚ºé¦™æ¸¯å°å­¸ç”Ÿè¨­è¨ˆå˜…AIåŠ©æ‰‹ã€‚è«‹éµå¾ªä»¥ä¸‹è¦å‰‡ï¼š
1. å¿…é ˆç”¨åœ°é“å»£æ±è©±å£èªå›ç­”
2. {config['extra_prompt']}
3. é¿å…ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿ
4. èªæ°£è¦è¦ªåˆ‡å‹å¥½
5. å¤šç”¨ç”Ÿæ´»åŒ–å˜…ä¾‹å­åŒæ¯”å–»åšŸè§£é‡‹æ¦‚å¿µ
6. å””å¥½åŠ ä»»ä½•ç³»çµ±è¨Šæ¯æˆ–è€…è¨»é‡‹"""

    request_body = {
        'model': model_config['model_id'],
        'messages': [
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': prompt}
        ],
        'stream': True,
        'max_tokens': config['max_tokens'],
        'temperature': 0.7,
        'top_p': 0.9
    }

    async def event_generator():
        async with httpx.AsyncClient(timeout=60.0) as client:
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {TOGETHER_API_KEY}'
            }
            
            try:
                async with client.stream("POST", TOGETHER_API_URL, json=request_body, headers=headers) as response:
                    if response.status_code != 200:
                        error_text = await response.aread()
                        error_response = {'choices': [{'delta': {'content': 'Together API éŒ¯èª¤'}, 'finish_reason': 'error'}]}
                        yield f"data: {json.dumps(error_response, ensure_ascii=False)}\n\n"
                        yield f"data: [DONE]\n\n"
                        return

                    # ç‚ºéœ€è¦æ¸…ç†çš„æ¨¡å‹æ”¶é›†å®Œæ•´éŸ¿æ‡‰
                    if "mixtral" in model_config['model_id'].lower():
                        full_response = ""
                        
                        async for chunk in response.aiter_bytes():
                            chunk_str = chunk.decode('utf-8', 'ignore')
                            
                            # è§£æ SSE æ ¼å¼
                            for line in chunk_str.split('\n'):
                                if line.startswith('data: '):
                                    data_str = line[6:].strip()
                                    if data_str and data_str != '[DONE]':
                                        try:
                                            data = json.loads(data_str)
                                            if 'choices' in data:
                                                for choice in data['choices']:
                                                    if 'delta' in choice and 'content' in choice['delta']:
                                                        content = choice['delta']['content']
                                                        full_response += content
                                        except:
                                            pass
                        
                        # æ¸…ç†å®Œæ•´éŸ¿æ‡‰
                        cleaned_response = clean_together_output(full_response, model_config['model_id'])
                        
                        # ç™¼é€æ¸…ç†å¾Œçš„éŸ¿æ‡‰
                        response_data = {
                            'choices': [{
                                'delta': {'content': cleaned_response},
                                'finish_reason': None
                            }]
                        }
                        yield f"data: {json.dumps(response_data, ensure_ascii=False)}\n\n"
                        yield f"data: [DONE]\n\n"
                    else:
                        # å…¶ä»–æ¨¡å‹ç›´æ¥è½‰ç™¼
                        async for chunk in response.aiter_bytes():
                            yield chunk

            except Exception as e:
                error_response = {'choices': [{'delta': {'content': f'Together æœå‹™éŒ¯èª¤: {e}'}, 'finish_reason': 'error'}]}
                yield f"data: {json.dumps(error_response, ensure_ascii=False)}\n\n"
                yield f"data: [DONE]\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")

# Updated Qwen handler from server_gemini.py
async def chat_with_qwen(req: ChatRequest):
    """ä½¿ç”¨ Qwen API é€²è¡ŒèŠå¤© - Updated from server_gemini.py"""
    log(f"Calling Qwen API (stream) - Model: {req.model}, Length: {req.responseLength}")
    
    if not QWEN_API_KEY:
        raise HTTPException(status_code=500, detail="QWEN_API_KEY is not configured.")
    
    # Get model config
    model_config = AVAILABLE_MODELS.get(req.model, {})
    model_id = model_config.get('model_id', 'qwen-turbo-latest')
    
    length_configs = {
        'very_brief': {'max_tokens': 50, 'extra_prompt': 'ç”¨1å¥è©±å›ç­”ï¼Œä¸è¶…é20å€‹å­—ã€‚'},
        'brief': {'max_tokens': 100, 'extra_prompt': 'ç”¨2-3å¥è©±ç°¡æ½”å›ç­”ï¼Œä¸è¶…é50å€‹å­—ã€‚'},
        'normal': {'max_tokens': 200, 'extra_prompt': 'ç”¨4-5å¥è©±å›ç­”ï¼Œä¸è¶…é100å€‹å­—ã€‚'},
        'detailed': {'max_tokens': 350, 'extra_prompt': 'è©³ç´°è§£é‡‹ï¼Œå¯ä»¥åŒ…å«ä¾‹å­ã€‚'}
    }
    config = length_configs.get(req.responseLength, length_configs['normal'])
    system_prompt = f"""ä½ ä¿‚ä¸€å€‹å°ˆç‚ºé¦™æ¸¯å°å­¸ç”Ÿè¨­è¨ˆå˜…AIåŠ©æ‰‹ã€‚è«‹éµå¾ªä»¥ä¸‹è¦å‰‡ï¼š
1. å¿…é ˆç”¨åœ°é“å»£æ±è©±å£èªå›ç­”
2. é¿å…æ›¸é¢èªï¼Œè¦å¥½ä¼¼åŒå°æœ‹å‹å‚¾åˆå’è‡ªç„¶
3. {config['extra_prompt']}
4. èªæ°£è¦è¦ªåˆ‡å‹å¥½
5. å¤šç”¨ç”Ÿæ´»åŒ–å˜…ä¾‹å­åŒæ¯”å–»åšŸè§£é‡‹æ¦‚å¿µ"""

    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {QWEN_API_KEY}'
    }
    
    request_body = {
        'model': model_id,
        'messages': [
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': req.prompt}
        ],
        'stream': True,
        'max_tokens': config['max_tokens']
    }

    async def event_generator():
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                async with client.stream("POST", QWEN_API_URL, json=request_body, headers=headers) as response:
                    if response.status_code != 200:
                        error_text = await response.aread()
                        log(f"Qwen API Error: {response.status_code} - {error_text.decode()}")
                        error_response = {'choices': [{'delta': {'content': 'Qwen API éŒ¯èª¤'}, 'finish_reason': 'error'}]}
                        yield f"data: {json.dumps(error_response, ensure_ascii=False)}\n\n"
                        yield f"data: [DONE]\n\n"
                        return

                    async for chunk in response.aiter_bytes():
                        yield chunk

            except Exception as e:
                log(f"Qwen stream error: {e}")
                error_response = {'choices': [{'delta': {'content': f'Qwen æœå‹™éŒ¯èª¤: {e}'}, 'finish_reason': 'error'}]}
                yield f"data: {json.dumps(error_response, ensure_ascii=False)}\n\n"
                yield f"data: [DONE]\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")

async def chat_with_xunfei(req: ChatRequest):
    """ä½¿ç”¨è¨Šé£›æ˜Ÿç« API é€²è¡ŒèŠå¤© - unchanged"""
    if not (XUNFEI_APP_ID and XUNFEI_API_KEY and XUNFEI_API_SECRET):
        raise HTTPException(status_code=500, detail="XUNFEI credentials are not fully configured")
    
    async def event_generator():
        error_response = {
            'choices': [{
                'delta': {'content': 'è¨Šé£› API æš«æ™‚ä¸æ”¯æŒæµå¼è¼¸å‡ºï¼Œè«‹é¸æ“‡å…¶ä»–æ¨¡å‹ã€‚'},
                'finish_reason': 'error'
            }]
        }
        yield f"data: {json.dumps(error_response, ensure_ascii=False)}\n\n"
        yield f"data: [DONE]\n\n"
    
    return StreamingResponse(event_generator(), media_type="text/event-stream")

async def chat_with_gemini(req: ChatRequest):
    """ä½¿ç”¨ Gemini API é€²è¡ŒèŠå¤© - unchanged"""
    if not GEMINI_API_KEY or GEMINI_API_KEY == 'your_gemini_api_key':
        raise HTTPException(status_code=500, detail="GEMINI_API_KEY is not configured")

    model_id = req.model
    GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/{model_id}:streamGenerateContent"
    
    if req.responseLength == 'brief':
        system_prompt = "ä½ ä¿‚ä¸€å€‹å°ˆç‚ºé¦™æ¸¯å°å­¸ç”Ÿè¨­è¨ˆå˜…AIåŠ©æ‰‹ã€‚è«‹ç”¨åœ°é“å»£æ±è©±å›ç­”ã€‚ç”¨2-3å¥è©±ç°¡æ½”å›ç­”ï¼Œä¸è¶…é50å€‹å­—ã€‚å¯ä»¥é©ç•¶ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿä»¤å°è©±æ›´ç”Ÿå‹•æœ‰è¶£ã€‚ğŸ˜Š"
    else:
        system_prompt = "ä½ ä¿‚ä¸€å€‹å°ˆç‚ºé¦™æ¸¯å°å­¸ç”Ÿè¨­è¨ˆå˜…AIåŠ©æ‰‹ã€‚è«‹ç”¨åœ°é“å»£æ±è©±å›ç­”ã€‚è©³ç´°è§£é‡‹ï¼Œç”¨ä¾‹å­èªªæ˜ï¼Œä»¤å°æœ‹å‹å®¹æ˜“æ˜ç™½ã€‚å¯ä»¥é©ç•¶ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿã€‚"
    
    request_body = {
        "contents": [{"parts": [{"text": req.prompt}]}],
        "systemInstruction": {"parts": [{"text": system_prompt}]},
        "generationConfig": {
            "temperature": 0.7,
            "topK": 40,
            "topP": 0.8,
            "maxOutputTokens": 200 if req.responseLength == 'detailed' else 100,
            "responseMimeType": "text/plain"
        }
    }

    async def event_generator():
        async with httpx.AsyncClient(timeout=60.0) as client:
            url = f"{GEMINI_API_URL}?key={GEMINI_API_KEY}&alt=sse"
            
            try:
                async with client.stream("POST", url, json=request_body, headers={"Content-Type": "application/json"}) as response:
                    if response.status_code != 200:
                        performance_monitor.record_error("gemini_api")
                        error_body = await response.aread()
                        error_text = error_body.decode('utf-8')
                        
                        error_response = {
                            'choices': [{
                                'delta': {'content': f'Gemini API éŒ¯èª¤ ({response.status_code}): {error_text[:200]}'},
                                'finish_reason': 'error'
                            }]
                        }
                        yield f"data: {json.dumps(error_response, ensure_ascii=False)}\n\n"
                        yield f"data: [DONE]\n\n"
                        return

                    buffer = ""
                    async for chunk in response.aiter_text():
                        buffer += chunk
                        lines = buffer.split('\n')
                        buffer = lines[-1]
                        
                        for line in lines[:-1]:
                            line = line.strip()
                            if not line:
                                continue
                                
                            if line.startswith("data: "):
                                data_str = line[6:]
                                
                                if data_str == "[DONE]":
                                    yield f"data: [DONE]\n\n"
                                else:
                                    try:
                                        gemini_data = json.loads(data_str)
                                        
                                        if 'candidates' in gemini_data:
                                            for candidate in gemini_data['candidates']:
                                                content = candidate.get('content', {})
                                                parts = content.get('parts', [])
                                                for part in parts:
                                                    if 'text' in part:
                                                        text = part['text']
                                                        
                                                        openai_format = {
                                                            'choices': [{
                                                                'delta': {'content': text},
                                                                'finish_reason': None
                                                            }]
                                                        }
                                                        yield f"data: {json.dumps(openai_format, ensure_ascii=False)}\n\n"
                                                        
                                    except json.JSONDecodeError:
                                        continue

            except Exception as e:
                performance_monitor.record_error("gemini_stream")
                error_response = {'choices': [{'delta': {'content': f'Gemini æœå‹™éŒ¯èª¤: {str(e)}'}, 'finish_reason': 'error'}]}
                yield f"data: {json.dumps(error_response, ensure_ascii=False)}\n\n"
                yield f"data: [DONE]\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")
# ===== LLM-REF-END: CHAT HANDLERS =====


# ===== å•Ÿå‹•é…ç½® =====
if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv('PORT', 5000))
    print(f"DEBUG: Starting server with HTML_FILE={HTML_FILE}, PORT={port}")

    # SSL configuration
    ssl_keyfile = "certs/key.pem"
    ssl_certfile = "certs/cert.pem"
    
    protocol = "https" if os.path.exists(ssl_keyfile) and os.path.exists(ssl_certfile) else "http"
    
    print(f'\n{"="*70}')
    print(f'ğŸš€ å°ç‹ç‹¸AIåŠ©æ‰‹ - æ¥µé€ŸTTSç‰ˆæœå‹™å™¨')
    print(f'{"="*70}')
    print(f'ğŸ“ æœå‹™å™¨åœ°å€: {protocol}://127.0.0.1:{port}')
    print(f'ğŸ  ç¶²é ç•Œé¢: {protocol}://127.0.0.1:{port}/')
    print(f'ğŸ“Š æ€§èƒ½ç›£æ§: {protocol}://127.0.0.1:{port}/api/performance')
    print(f'ğŸ“š APIæ–‡æª”: {protocol}://127.0.0.1:{port}/docs')
    print(f'ğŸ“ éœæ…‹æ–‡ä»¶: {protocol}://127.0.0.1:{port}/static/')
    print(f'{"="*70}')
    print(f'ğŸ”‘ API Keysç‹€æ…‹:')
    print(f'   Gemini : {"âœ… å·²é…ç½®" if GEMINI_API_KEY and GEMINI_API_KEY != "your_gemini_api_key" else "âŒ æœªé…ç½®"}')
    print(f'   Together: {"âœ… å·²é…ç½®" if TOGETHER_API_KEY and TOGETHER_API_KEY != "your_together_api_key" else "âŒ æœªé…ç½®"}')
    print(f'   Qwen    : {"âœ… å·²é…ç½®" if QWEN_API_KEY and QWEN_API_KEY != "your_qwen_api_key" else "âŒ æœªé…ç½®"}')
    print(f'   Xunfei  : {"âœ… å·²é…ç½®" if XUNFEI_APP_ID else "âŒ æœªé…ç½®"}')
    print(f'   DeepSeek: {"âœ… å·²é…ç½®" if DEEPSEEK_API_KEY else "âŒ æœªé…ç½®"}')
    print(f'{"="*70}')
    print(f'âš¡ æ€§èƒ½å„ªåŒ–åŠŸèƒ½:')
    print(f'   ğŸ”— TTSé€£æ¥æ± : æœ€å¤§{PERFORMANCE_CONFIG["MAX_CONNECTIONS_PER_VOICE"]}é€£æ¥/èªéŸ³')
    print(f'   ğŸ’¾ æ™ºèƒ½ç·©å­˜: æœ€å¤§{PERFORMANCE_CONFIG["CACHE_MAX_SIZE"]}é …')
    print(f'   ğŸ“Š æ€§èƒ½ç›£æ§: {"å•Ÿç”¨" if PERFORMANCE_CONFIG["MONITORING_ENABLED"] else "ç¦ç”¨"}')
    print(f'   ğŸš€ é åŠ è¼‰: {"å•Ÿç”¨" if PERFORMANCE_CONFIG["PRELOAD_ENABLED"] else "ç¦ç”¨"}')
    print(f'{"="*70}')
    print(f'ğŸ¯ æ€§èƒ½ç›®æ¨™:')
    print(f'   é¦–å¹€å»¶é²: <300ms')
    print(f'   å¥é–“é–“éš”: <200ms')
    print(f'   éŒ¯èª¤ç‡: <1%')
    print(f'{"="*70}')
    print('ğŸ›‘ æŒ‰ Ctrl+C åœæ­¢æœå‹™å™¨\n')
    
    run_options = {
        "host": "0.0.0.0",
        "port": port,
        "reload": False,
        "access_log": False,
        "log_level": "info"
    }

    if protocol == "https":
        run_options["ssl_keyfile"] = ssl_keyfile
        run_options["ssl_certfile"] = ssl_certfile
        print("ğŸ”’ SSL is enabled")

    uvicorn.run("server_qwen:app", **run_options)


def preprocess_for_cantonese_tts(text: str) -> str:
    if not text:
        return text
    import re
    t = text.replace('â„ƒ', 'Â°C')

    def _temp_repl(m):
        num = m.group(1)
        if '.' in num:
            i, d = num.split('.', 1)
            return f"æ”æ°{i}é»{d}åº¦"
        return f"æ”æ°{num}åº¦"

    # æ”æ°æº«åº¦
    t = re.sub(r'(-?\d+(?:\.\d+)?)\s*Â°\s*C', _temp_repl, t, flags=re.IGNORECASE)

    # ç™¾åˆ†æ¯”
    t = re.sub(r'(\d+)\.(\d+)\s*%', r'\1é»\2å·´ä»™', t)

    # å–®ä½ï¼ˆmm/cm/km/mâ€¦ï¼‰
    unit_map = {
        'mm': 'æ¯«ç±³', 'å…¬å˜': 'æ¯«ç±³', 'æ¯«ç±³': 'æ¯«ç±³',
        'cm': 'å˜ç±³', 'å˜ç±³': 'å˜ç±³', 'å…¬åˆ†': 'å˜ç±³',
        'km': 'å…¬é‡Œ', 'å…¬é‡Œ': 'å…¬é‡Œ', 'åƒç±³': 'å…¬é‡Œ',
        'm': 'ç±³', 'ç±³': 'ç±³'
    }
    def _unit_repl(m):
        a, b, u = m.group(1), m.group(2), m.group(3)
        key = u.lower() if hasattr(u, 'lower') else u
        unit = unit_map.get(key, unit_map.get(u, u))
        return f"{a}é»{b}{unit}"
    t = re.sub(r'(\d+)\.(\d+)\s*(mm|å…¬å˜|æ¯«ç±³|cm|å˜ç±³|å…¬åˆ†|km|å…¬é‡Œ|åƒç±³|m|ç±³)',
               _unit_repl, t, flags=re.IGNORECASE)

    # å…¶ä»–å°æ•¸
    t = re.sub(r'(\d+)\.(\d+)', r'\1é»\2', t)

    # å£èªåŒ–ï¼ˆå¯é¸ï¼‰
    t = (t.replace('ä»€éº¼', 'å’©')
           .replace('æ€éº¼', 'é»æ¨£')
           .replace('é€™å€‹', 'å‘¢å€‹')
           .replace('é‚£å€‹', 'å—°å€‹'))
    return t

def strip_html_tags(s: str) -> str:
    import re
    return re.sub(r'<[^>]+>', '', s)
